\contentsline {chapter}{About this Book}{6}{chapter*.2}
\contentsline {section}{\numberline {0.1}How to Use this Book}{6}{section.0.1}
\contentsline {section}{\numberline {0.2}Why Another Textbook?}{6}{section.0.2}
\contentsline {section}{\numberline {0.3}Organization and Auxilary Material}{7}{section.0.3}
\contentsline {section}{\numberline {0.4}Acknowledgements}{7}{section.0.4}
\contentsline {chapter}{\numberline {1}Decision Trees}{8}{chapter.1}
\contentsline {section}{\numberline {1.1}What Does it Mean to Learn?}{8}{section.1.1}
\contentsline {section}{\numberline {1.2}Some Canonical Learning Problems}{10}{section.1.2}
\contentsline {section}{\numberline {1.3}The Decision Tree Model of Learning}{10}{section.1.3}
\contentsline {section}{\numberline {1.4}Formalizing the Learning Problem}{14}{section.1.4}
\contentsline {section}{\numberline {1.5}Inductive Bias: What We Know Before the Data Arrives}{16}{section.1.5}
\contentsline {section}{\numberline {1.6}Not Everything is Learnable}{17}{section.1.6}
\contentsline {section}{\numberline {1.7}Underfitting and Overfitting}{19}{section.1.7}
\contentsline {section}{\numberline {1.8}Separation of Training and Test Data}{20}{section.1.8}
\contentsline {section}{\numberline {1.9}Models, Parameters and Hyperparameters}{21}{section.1.9}
\contentsline {section}{\numberline {1.10}Chapter Summary and Outlook}{23}{section.1.10}
\contentsline {section}{\numberline {1.11}Exercises}{23}{section.1.11}
\contentsline {chapter}{\numberline {2}Geometry and Nearest Neighbors}{24}{chapter.2}
\contentsline {section}{\numberline {2.1}From Data to Feature Vectors}{24}{section.2.1}
\contentsline {section}{\numberline {2.2}K-Nearest Neighbors}{26}{section.2.2}
\contentsline {section}{\numberline {2.3}Decision Boundaries}{29}{section.2.3}
\contentsline {section}{\numberline {2.4}K-Means Clustering}{30}{section.2.4}
\contentsline {section}{\numberline {2.5}Warning: High Dimensions are Scary}{32}{section.2.5}
\contentsline {section}{\numberline {2.6}Extensions to KNN}{35}{section.2.6}
\contentsline {section}{\numberline {2.7}Exercises}{36}{section.2.7}
\contentsline {chapter}{\numberline {3}The Perceptron}{37}{chapter.3}
\contentsline {section}{\numberline {3.1}Bio-inspired Learning}{37}{section.3.1}
\contentsline {section}{\numberline {3.2}Error-Driven Updating: The Perceptron Algorithm}{38}{section.3.2}
\contentsline {section}{\numberline {3.3}Geometric Intrepretation}{41}{section.3.3}
\contentsline {section}{\numberline {3.4}Interpreting Perceptron Weights}{43}{section.3.4}
\contentsline {section}{\numberline {3.5}Perceptron Convergence and Linear Separability}{43}{section.3.5}
\contentsline {section}{\numberline {3.6}Improved Generalization: Voting and Averaging}{46}{section.3.6}
\contentsline {section}{\numberline {3.7}Limitations of the Perceptron}{49}{section.3.7}
\contentsline {section}{\numberline {3.8}Exercises}{50}{section.3.8}
\contentsline {chapter}{\numberline {4}Practical Issues}{51}{chapter.4}
\contentsline {section}{\numberline {4.1}The Importance of Good Features}{51}{section.4.1}
\contentsline {section}{\numberline {4.2}Irrelevant and Redundant Features}{52}{section.4.2}
\contentsline {section}{\numberline {4.3}Feature Pruning and Normalization}{54}{section.4.3}
\contentsline {section}{\numberline {4.4}Combinatorial Feature Explosion}{57}{section.4.4}
\contentsline {section}{\numberline {4.5}Evaluating Model Performance}{58}{section.4.5}
\contentsline {section}{\numberline {4.6}Cross Validation}{60}{section.4.6}
\contentsline {section}{\numberline {4.7}Hypothesis Testing and Statistical Significance}{63}{section.4.7}
\contentsline {section}{\numberline {4.8}Debugging Learning Algorithms}{66}{section.4.8}
\contentsline {section}{\numberline {4.9}Exercises}{67}{section.4.9}
\contentsline {chapter}{\numberline {5}Beyond Binary Classification}{68}{chapter.5}
\contentsline {section}{\numberline {5.1}Learning with Imbalanced Data}{68}{section.5.1}
\contentsline {section}{\numberline {5.2}Multiclass Classification}{72}{section.5.2}
\contentsline {section}{\numberline {5.3}Ranking}{76}{section.5.3}
\contentsline {section}{\numberline {5.4}Collective Classification}{80}{section.5.4}
\contentsline {section}{\numberline {5.5}Exercises}{83}{section.5.5}
\contentsline {chapter}{\numberline {6}Linear Models}{84}{chapter.6}
\contentsline {section}{\numberline {6.1}The Optimization Framework for Linear Models}{84}{section.6.1}
\contentsline {section}{\numberline {6.2}Convex Surrogate Loss Functions}{86}{section.6.2}
\contentsline {section}{\numberline {6.3}Weight Regularization}{88}{section.6.3}
\contentsline {section}{\numberline {6.4}Optimization with Gradient Descent}{90}{section.6.4}
\contentsline {section}{\numberline {6.5}From Gradients to Subgradients}{92}{section.6.5}
\contentsline {section}{\numberline {6.6}Closed-form Optimization for Squared Loss}{94}{section.6.6}
\contentsline {section}{\numberline {6.7}Support Vector Machines}{96}{section.6.7}
\contentsline {section}{\numberline {6.8}Exercises}{100}{section.6.8}
\contentsline {chapter}{\numberline {7}Probabilistic Modeling}{101}{chapter.7}
\contentsline {section}{\numberline {7.1}Classification by Density Estimation}{101}{section.7.1}
\contentsline {section}{\numberline {7.2}Statistical Estimation}{103}{section.7.2}
\contentsline {section}{\numberline {7.3}Naive Bayes Models}{105}{section.7.3}
\contentsline {section}{\numberline {7.4}Prediction}{107}{section.7.4}
\contentsline {section}{\numberline {7.5}Generative Stories}{108}{section.7.5}
\contentsline {section}{\numberline {7.6}Conditional Models}{109}{section.7.6}
\contentsline {section}{\numberline {7.7}Regularization via Priors}{111}{section.7.7}
\contentsline {section}{\numberline {7.8}Exercises}{113}{section.7.8}
\contentsline {chapter}{\numberline {8}Neural Networks}{114}{chapter.8}
\contentsline {section}{\numberline {8.1}Bio-inspired Multi-Layer Networks}{114}{section.8.1}
\contentsline {section}{\numberline {8.2}The Back-propagation Algorithm}{117}{section.8.2}
\contentsline {section}{\numberline {8.3}Initialization and Convergence of Neural Networks}{120}{section.8.3}
\contentsline {section}{\numberline {8.4}Beyond Two Layers}{121}{section.8.4}
\contentsline {section}{\numberline {8.5}Breadth versus Depth}{123}{section.8.5}
\contentsline {section}{\numberline {8.6}Basis Functions}{124}{section.8.6}
\contentsline {section}{\numberline {8.7}Exercises}{125}{section.8.7}
\contentsline {chapter}{\numberline {9}Kernel Methods}{126}{chapter.9}
\contentsline {section}{\numberline {9.1}From Feature Combinations to Kernels}{126}{section.9.1}
\contentsline {section}{\numberline {9.2}Kernelized Perceptron}{127}{section.9.2}
\contentsline {section}{\numberline {9.3}Kernelized K-means}{129}{section.9.3}
\contentsline {section}{\numberline {9.4}What Makes a Kernel}{130}{section.9.4}
\contentsline {section}{\numberline {9.5}Support Vector Machines}{133}{section.9.5}
\contentsline {section}{\numberline {9.6}Understanding Support Vector Machines}{136}{section.9.6}
\contentsline {section}{\numberline {9.7}Exercises}{138}{section.9.7}
\contentsline {chapter}{\numberline {10}Learning Theory}{139}{chapter.10}
\contentsline {section}{\numberline {10.1}The Role of Theory}{139}{section.10.1}
\contentsline {section}{\numberline {10.2}Induction is Impossible}{140}{section.10.2}
\contentsline {section}{\numberline {10.3}Probably Approximately Correct Learning}{141}{section.10.3}
\contentsline {section}{\numberline {10.4}PAC Learning of Conjunctions}{142}{section.10.4}
\contentsline {section}{\numberline {10.5}Occam's Razor: Simple Solutions Generalize}{145}{section.10.5}
\contentsline {section}{\numberline {10.6}Complexity of Infinite Hypothesis Spaces}{146}{section.10.6}
\contentsline {section}{\numberline {10.7}Learning with Noise}{148}{section.10.7}
\contentsline {section}{\numberline {10.8}Agnostic Learning}{148}{section.10.8}
\contentsline {section}{\numberline {10.9}Error versus Regret}{148}{section.10.9}
\contentsline {section}{\numberline {10.10}Exercises}{149}{section.10.10}
\contentsline {chapter}{\numberline {11}Ensemble Methods}{150}{chapter.11}
\contentsline {section}{\numberline {11.1}Voting Multiple Classifiers}{150}{section.11.1}
\contentsline {section}{\numberline {11.2}Boosting Weak Learners}{152}{section.11.2}
\contentsline {section}{\numberline {11.3}Random Ensembles}{155}{section.11.3}
\contentsline {section}{\numberline {11.4}Exercises}{156}{section.11.4}
\contentsline {chapter}{\numberline {12}Efficient Learning}{157}{chapter.12}
\contentsline {section}{\numberline {12.1}What Does it Mean to be Fast?}{157}{section.12.1}
\contentsline {section}{\numberline {12.2}Stochastic Optimization}{158}{section.12.2}
\contentsline {section}{\numberline {12.3}Sparse Regularization}{160}{section.12.3}
\contentsline {section}{\numberline {12.4}Feature Hashing}{162}{section.12.4}
\contentsline {section}{\numberline {12.5}Exercises}{163}{section.12.5}
\contentsline {chapter}{\numberline {13}Unsupervised Learning}{164}{chapter.13}
\contentsline {section}{\numberline {13.1}K-Means Clustering, Revisited}{164}{section.13.1}
\contentsline {section}{\numberline {13.2}Linear Dimensionality Reduction}{168}{section.13.2}
\contentsline {section}{\numberline {13.3}Manifolds and Graphs}{171}{section.13.3}
\contentsline {section}{\numberline {13.4}Non-linear Dimensionality Reduction}{171}{section.13.4}
\contentsline {section}{\numberline {13.5}Non-linear Clustering: Spectral Methods}{171}{section.13.5}
\contentsline {section}{\numberline {13.6}Exercises}{172}{section.13.6}
\contentsline {chapter}{\numberline {14}Expectation Maximization}{173}{chapter.14}
\contentsline {section}{\numberline {14.1}Clustering with a Mixture of Gaussians}{173}{section.14.1}
\contentsline {section}{\numberline {14.2}The Expectation Maximization Framework}{176}{section.14.2}
\contentsline {section}{\numberline {14.3}EM versus Gradient Descent}{178}{section.14.3}
\contentsline {section}{\numberline {14.4}Dimensionality Reduction with Probabilistic PCA}{178}{section.14.4}
\contentsline {section}{\numberline {14.5}Exercises}{178}{section.14.5}
\contentsline {chapter}{\numberline {15}Semi-Supervised Learning}{179}{chapter.15}
\contentsline {section}{\numberline {15.1}EM for Semi-Supervised Learning}{179}{section.15.1}
\contentsline {section}{\numberline {15.2}Graph-based Semi-Supervised Learning}{179}{section.15.2}
\contentsline {section}{\numberline {15.3}Loss-based Semi-Supervised Learning}{179}{section.15.3}
\contentsline {section}{\numberline {15.4}Active Learning}{180}{section.15.4}
\contentsline {section}{\numberline {15.5}Dangers of Semi-Supervised Learing}{180}{section.15.5}
\contentsline {section}{\numberline {15.6}Exercises}{180}{section.15.6}
\contentsline {chapter}{\numberline {16}Graphical Models}{181}{chapter.16}
\contentsline {section}{\numberline {16.1}Exercises}{181}{section.16.1}
\contentsline {chapter}{\numberline {17}Online Learning}{182}{chapter.17}
\contentsline {section}{\numberline {17.1}Online Learning Framework}{182}{section.17.1}
\contentsline {section}{\numberline {17.2}Learning with Features}{182}{section.17.2}
\contentsline {section}{\numberline {17.3}Passive Agressive Learning}{182}{section.17.3}
\contentsline {section}{\numberline {17.4}Learning with Lots of Irrelevant Features}{183}{section.17.4}
\contentsline {section}{\numberline {17.5}Exercises}{183}{section.17.5}
\contentsline {chapter}{\numberline {18}Structured Learning Tasks}{184}{chapter.18}
\contentsline {section}{\numberline {18.1}Exercises}{184}{section.18.1}
\contentsline {chapter}{\numberline {19}Bayesian Learning}{185}{chapter.19}
\contentsline {section}{\numberline {19.1}Exercises}{185}{section.19.1}
\contentsline {chapter}{Code and Datasets}{186}{appendix*.3}
\contentsline {chapter}{Notation}{187}{appendix*.4}
\contentsline {chapter}{Bibliography}{188}{appendix*.5}
\contentsline {chapter}{Index}{189}{appendix*.6}
\contentsfinish 
